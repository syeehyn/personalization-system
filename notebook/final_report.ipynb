{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IEOR 4571 Fall 2020 Homework2 Report**\n",
    "\n",
    "\n",
    "- Hu, Bo (uni: bh2569)\n",
    "- Qin, Rui (uni: rq217)\n",
    "- Yuan, Shuibenyang (uni: sy2938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI address http://127.0.0.1:4041\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "from src.utils import loading, Spark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# create spark session\n",
    "spark = Spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "**Objective function**\n",
    "\n",
    "The goal is to create lists of movies that users might be interested in. \n",
    "\n",
    "**Metrics**\n",
    "\n",
    "* Root Mean Square Error (RMSE): $RMSE = \\sqrt{\\frac{\\sum_{(u, j) \\in E}^{} e_{uj}^2}{\\lvert E \\rvert}}$, where $e_{ij} = \\hat{r}_{uj} - r_{uj}$.\n",
    "\n",
    "* Accuracy: ratio of ratings such that both predicted value and actual value larger or equal to 3 or both predicted value and actual value less than 3.\n",
    "\n",
    "* Coverage: ratio of users or movies whose number of both predicted values and actual values larger or equal to 3 larger or equal to k.\n",
    "\n",
    "**Intended users**\n",
    "\n",
    "The recommendation system is created for the general audience so that everyone who enjoys movies benefits from the system.\n",
    "\n",
    "**Business rules**\n",
    "\n",
    "In order to keep the user entertained rather than only focusing on what they already know, one business rule we come up with is to include at least two different genres when recommending k movies even though the predicted rating might be low. Since our business goal is to benefit all movie lovers, we believe letting them expose to as many genres as possible is fundamental.\n",
    "\n",
    "**Performance requirements**\n",
    "\n",
    "For performance, we would like to serve the online queries in real time. For model based algorithms, it’s fine to train it offline and then serve the model online. However, for memory based models, it is hard to meet the performance requirements unless implementing more sophisticated algorithms like approximate nearest neighbor. \n",
    "\n",
    "In order to interpret models better, we decide to make the matrix factorization algorithm to only produce non-negative matrices. In that case, we would be able to identify some elements that are important for the algorithm to learn users’ behaviours (higher value in the matrices would produce higher ratings)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sample\n",
    "\n",
    "**sampling methodology**\n",
    "\n",
    "We perform sampling w.r.t Conditional Matrix Sampling, in which, we will sample the matrix of $M$ user indices and $N$ movie indices filtering out users who do not have at least $i$ ratings and movies which do not have at least $j$ ratings. If numbers of users and movies are not meet the minimal requirements $M$ and $N$, we will keep sampling process with increased matrix indices number of user and movie until users and movies meet minimal requirements $M$ and $N$.\n",
    "\n",
    "**generating sample**\n",
    "\n",
    "to generate sample, run following lines (The sample has been already stored under `[project_directory]/raw/sample.csv`):\n",
    "\n",
    "```bash\n",
    "cd [project_directory]\n",
    "python main.py download sample\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data from '/data/raw/sample.csv'\n",
    "datas = loading(spark, '../data/raw')\n",
    "sample = datas['sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            number of data points in the sample: 444289,\n",
      "            number of unique users in the sample: 20000,\n",
      "            number of unique movies in the sample: 1000,\n",
      "            mean of number of movies a user rated:22.21445,\n",
      "            mean of number of users a movie be rated: 444.289,\n",
      "            average rating: 3.562069958968149,\n",
      "            standard deviation of rating: 1.0467842419621054,\n",
      "            average rating by user: 3.685826601448731,\n",
      "            standard deviation of rating by user mean: 0.5225862364055156,\n",
      "            average rating by movie: 3.290922029378176,\n",
      "            standard deviation of rating by movie mean: 0.5133362661920545\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "            number of data points in the sample: {sample.count()},\n",
    "            number of unique users in the sample: {sample.select('userId').distinct().count()},\n",
    "            number of unique movies in the sample: {sample.select('movieId').distinct().count()},\n",
    "            mean of number of movies a user rated:{sample.groupby('userId').agg(F.count('movieId').alias('cnt')).select(F.mean('cnt')).collect()[0][0]},\n",
    "            mean of number of users a movie be rated: {sample.groupby('movieId').agg(F.count('userId').alias('cnt')).select(F.mean('cnt')).collect()[0][0]},\n",
    "            average rating: {sample.select(F.mean('rating')).collect()[0][0]},\n",
    "            standard deviation of rating: {sample.select(F.stddev('rating')).collect()[0][0]},\n",
    "            average rating by user: {sample.groupby('userId').agg(F.mean('rating').alias('rating')).select(F.mean('rating')).collect()[0][0]},\n",
    "            standard deviation of rating by user mean: {sample.groupby('userId').agg(F.mean('rating').alias('rating')).select(F.stddev('rating')).collect()[0][0]},\n",
    "            average rating by movie: {sample.groupby('movieId').agg(F.mean('rating').alias('rating')).select(F.mean('rating')).collect()[0][0]},\n",
    "            standard deviation of rating by movie mean: {sample.groupby('movieId').agg(F.mean('rating').alias('rating')).select(F.stddev('rating')).collect()[0][0]}\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We implemented two types of collaborative filtering techniques:\n",
    "\n",
    "- Memory Based Collaborative Filtering\n",
    "    \n",
    "- Model Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model\n",
    "\n",
    "**usage**\n",
    "\n",
    "The source code of Baseline Model is under the file directory: `[project_directory]/src/baseline/baseline.py`\n",
    "\n",
    "To use the predictor, `import Baseline() from src.baseline`\n",
    "\n",
    "```python\n",
    "###example\n",
    "#train_schema: DataFrame[userId: string, movieId: string, rating: string]\n",
    "#test_schema: DataFrame[userId: string, movieId: string, rating: string]\n",
    "from src.baseline import Baseline\n",
    "model = Baseline(base='user', usercol='userId', itemcol='movieId', ratingcol='rating')\n",
    "model.fit(train)\n",
    "prediction = model.predict(test)\n",
    "#prediction_schema: DataFrame[userId: string, movieId: string, rating: float, prediction: float]\n",
    "   \n",
    "```\n",
    "\n",
    "**implementation details**\n",
    "\n",
    "The baseline model makes simple prediction by taking mean of user mean and item mean:\n",
    "\n",
    "$R$ is defined as $R_{u,i} = \\frac{\\mu_u + \\mu_i}{2}$ where u and i represents user and item accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.baseline import Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Based Collaborative Filtering\n",
    "\n",
    "**usage**\n",
    "\n",
    "The source code of Memory Based Collaborative Filtering is under the file directory: `[project_directory]/src/memory_based/memory_based_cf.py`. \n",
    "\n",
    "To use the predictor, `import Memory_based_CF() from src.memory_based`\n",
    "\n",
    "```python\n",
    "###example\n",
    "#spark: the spark session\n",
    "#train_schema: DataFrame[userId: string, movieId: string, rating: string]\n",
    "#test_schema: DataFrame[userId: string, movieId: string, rating: string]\n",
    "from src.memory_based import Memory_based_CF\n",
    "#user based\n",
    "model = Memory_based_CF(spark, base='user', usercol='userId', itemcol='movieId', ratingcol='rating')\n",
    "model.fit(train)\n",
    "prediction = model.predict(test)\n",
    "#item based\n",
    "model = Memory_based_CF(spark, base='item', usercol='userId', itemcol='movieId', ratingcol='rating')\n",
    "model.fit(train)\n",
    "prediction = model.predict(test)\n",
    "#prediction_schema: DataFrame[userId: string, movieId: string, rating: float, prediction: float]\n",
    "   \n",
    "```\n",
    "\n",
    "\n",
    "**implementation details**\n",
    "\n",
    "The data first transformed into sparse matrix representation, (user by item) if user based and (item by user) if item based.\n",
    "\n",
    "The the prediction matrix $R$ is trained with following formula:\n",
    "\n",
    "$R$ is defined as $R_{i, j} = \\mu_i + \\frac{\\sum_{v\\in P_i(j)}S(i, v)\\cdot (r_{vj} - \\mu_v)}{\\sum_{v\\in P_i(j)}|S(i, v)|}$\n",
    "\n",
    "where $S$ is the Pearson Similarity Matrix\n",
    "\n",
    "$S$ is defined as $S_{u,v} = \\frac{\\sum_{k\\in I_u \\cap I_v}(r_{uk} - \\mu_u)(r_{vk} - \\mu_v)}{\\sqrt{\\sum_{k\\in I_u \\cap I_v}(r_{uk} - \\mu_u)^2}\\sqrt{\\sum_{k \\in I_u \\cap I_v}(r_{vk} - \\mu_v)^2}}$\n",
    "\n",
    "The algorithm is implemented with numpy array (for prediction) and scipy csr sparse matrix (for training). \n",
    "\n",
    "Every operation uses numpy matrix operations (aka. dot product, norm, etc) which optimizes the computational speed by trading off extra memories (for loop takes $\\approx 10$ minutes to train and matrix operations takes $\\approx 1$ minutes to train for our experimental sample in user based CF).\n",
    "\n",
    "**user based collabrative filtering**\n",
    "\n",
    "When R is (user by item) and S is (user by user), it is User Based Collabrative Filtering\n",
    "\n",
    "**user based collabrative filtering**\n",
    "\n",
    "When R is (item by user) and S is (item by item), it is Item Based Collabrative Filtering\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.memory_based import Memory_based_CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based Collaborative Filtering\n",
    "\n",
    "**usage**\n",
    "\n",
    "The source code of Memory Based Collaborative Filtering is under the file directory: `[project_directory]/src/model_based/als.py`. \n",
    "\n",
    "To use the predictor, just `import Als() from src.model_based`\n",
    "\n",
    "```python\n",
    "###example\n",
    "#train_schema: DataFrame[userId: string, movieId: string, rating: string]\n",
    "#test_schema: DataFrame[userId: string, movieId: string, rating: string]\n",
    "\n",
    "from src.model_based import Als\n",
    "\n",
    "model = Als(userCol='userId', itemCol='movieId', ratingCol='rating', regParam=.01, seed=0, rank=10)\n",
    "model.fit(train)\n",
    "prediction = model.predict(test)\n",
    "\n",
    "#prediction_schema: DataFrame[userId: string, movieId: string, rating: float, prediction: float]\n",
    "   \n",
    "```\n",
    "\n",
    "\n",
    "**implementation detail**\n",
    "\n",
    "The data first caseted userId and movieId into integers and then fit into `pyspark.ml.recommendation.ALS`.\n",
    "\n",
    "Our implementation takes advantages of model based collaborative filtering algorithm implemented in `spark.ml`, in which users and products are described by a small set of latent factors that can be used to predict missing entries `spark.ml` uses the alternating least squares (ALS) algorithm to learn these latent factors.\n",
    "\n",
    "Since there are many parameters in ALS of `spark.ml`, we will fix `nonnegative = True` since negative rating is not possible in our scenario, and we will only select `regParam`(scale of regulization term) and `rank`(number of hidden factors) to be tuned. (We also tried to tune `maxIter` parameter, but when `maxIter > 20` will blow up memory in our machine with large `rank`, and it takes much longer with nearly the same results, so we will keep `maxIter` with default `=10`.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.model_based import Als"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Setup\n",
    "\n",
    "**general setup**\n",
    "\n",
    "we will first setup a fixed splited dataset for all models: baseline, memory-based CF, and model-based CF.\n",
    "\n",
    "With following splits based on every user:\n",
    "- train, test : $75\\%, 25\\%$\n",
    "- train, test : $50\\%, 50\\%$\n",
    "- train, test : $25\\%, 25\\%$\n",
    "\n",
    "For example, for (train, test: $75\\%, 25\\%$), we will retain first (older to newer) $75\\%$ of every user's rating w.r.t their rating time as train set, and we will take the rest as test set.\n",
    "\n",
    "**model based CF**\n",
    "\n",
    "For model based CF, we will do additional hyper-parameter tuning with $10\\%$ validation set from train set to get the best possible parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import Evaluator, Cross_validate_als\n",
    "from src.model_based import Als\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_0.5_0.5', 'train_0.25_0.75', 'test_0.5_0.5', 'test_0.25_0.75', 'train_0.75_0.25', 'test_0.75_0.25']\n"
     ]
    }
   ],
   "source": [
    "splits = loading(spark, '../data/interim')\n",
    "print(list(splits.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build evaluation pipeline\n",
    "def evaluate(train, test, evaluators, model):\n",
    "    print('training')\n",
    "    start = time.time()\n",
    "    model.fit(train)\n",
    "    end = time.time()\n",
    "    print(f'training time: {round(end - start, 2)} seconds')\n",
    "    print('inferencing train set')\n",
    "    start = time.time()\n",
    "    train_pred = model.predict(train)\n",
    "    end = time.time()\n",
    "    print(f'inference time: {round(end - start, 2)} seconds')\n",
    "    print('inferencing test set')\n",
    "    start = time.time()\n",
    "    test_pred = model.predict(test)\n",
    "    end = time.time()\n",
    "    print(f'inference time: {round(end - start, 2)} seconds')\n",
    "    res = pd.DataFrame(np.zeros((len(evaluators),2)), columns = ['train', 'test'], index = evaluators.keys())\n",
    "    for eva in evaluators.keys():\n",
    "        res.loc[eva, 'train'] = evaluators[eva].evaluate(train_pred)\n",
    "        res.loc[eva, 'test'] = evaluators[eva].evaluate(test_pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics\n",
    "\n",
    "Our metrics are as follow:\n",
    "\n",
    "* Root Mean Square Error (RMSE): $RMSE = \\sqrt{\\frac{\\sum_{(u, j) \\in E}^{} e_{uj}^2}{\\lvert E \\rvert}}$, where $e_{ij} = \\hat{r}_{uj} - r_{uj}$.\n",
    "\n",
    "* Accuracy: ratio of ratings such that both predicted value and actual value larger or equal to 3 or both predicted value and actual value less than 3.\n",
    "\n",
    "* Coverage: ratio of users or movies whose number of both predicted values and actual values larger or equal to 3 larger or equal to k.\n",
    "\n",
    "**choice of k in coverage**\n",
    "\n",
    "Since mean of number of movies a user rated:22.21445 and mean of number of users a movie be rated: 444.289, We will choose $k = 10$ for user coverage and $k = 100$ for item coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {'rmse': Evaluator(metrics = 'rmse'), \n",
    "              'accuracy': Evaluator(metrics = 'accuracy'), \n",
    "              'coverage_2': Evaluator(metrics = 'converage_k', \n",
    "                                       ratingCol='rating', \n",
    "                                       predCol='prediction', \n",
    "                                       idCol='userId', \n",
    "                                       k=2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Baseline()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "training time: 0.12 seconds\n",
      "inferencing train set\n",
      "inference time: 0.05 seconds\n",
      "inferencing test set\n",
      "inference time: 0.04 seconds\n",
      "training\n",
      "training time: 0.03 seconds\n",
      "inferencing train set\n",
      "inference time: 0.03 seconds\n",
      "inferencing test set\n",
      "inference time: 0.03 seconds\n",
      "training\n",
      "training time: 0.03 seconds\n",
      "inferencing train set\n",
      "inference time: 0.03 seconds\n",
      "inferencing test set\n",
      "inference time: 0.03 seconds\n",
      "CPU times: user 102 ms, sys: 36 ms, total: 138 ms\n",
      "Wall time: 21.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for i in ['0.75_0.25', '0.5_0.5', '0.25_0.75']:\n",
    "    train, test = splits['train_' + i], splits['test_' + i]\n",
    "    res = evaluate(train, test, evaluators, model)\n",
    "    result.append(res)\n",
    "baseline_res = pd.DataFrame(index=['rmse', 'accuracy', 'coverage_5'], columns = ['train', 'test', 'split'])\n",
    "for i, j in zip(result, ['0.75_0.25', '0.5_0.5', '0.25_0.75']):\n",
    "    i['split'] = j\n",
    "    baseline_res = baseline_res.append(i)\n",
    "baseline_res = baseline_res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_res.to_csv('../data/processed/baseline_res.csv', header = True, index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.871576</td>\n",
       "      <td>0.913753</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.859188</td>\n",
       "      <td>0.831457</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.991162</td>\n",
       "      <td>0.716097</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.858781</td>\n",
       "      <td>0.922455</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.865157</td>\n",
       "      <td>0.832447</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.979832</td>\n",
       "      <td>0.962492</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.832999</td>\n",
       "      <td>0.937656</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.874522</td>\n",
       "      <td>0.829091</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.911197</td>\n",
       "      <td>0.981760</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train      test      split\n",
       "rmse        0.871576  0.913753  0.75_0.25\n",
       "accuracy    0.859188  0.831457  0.75_0.25\n",
       "coverage_2  0.991162  0.716097  0.75_0.25\n",
       "rmse        0.858781  0.922455    0.5_0.5\n",
       "accuracy    0.865157  0.832447    0.5_0.5\n",
       "coverage_2  0.979832  0.962492    0.5_0.5\n",
       "rmse        0.832999  0.937656  0.25_0.75\n",
       "accuracy    0.874522  0.829091  0.25_0.75\n",
       "coverage_2  0.911197  0.981760  0.25_0.75"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "baseline_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Memory_based_CF(spark, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "training time: 38.95 seconds\n",
      "inferencing train set\n",
      "inference time: 10.73 seconds\n",
      "inferencing test set\n",
      "inference time: 3.67 seconds\n",
      "training\n",
      "training time: 34.61 seconds\n",
      "inferencing train set\n",
      "inference time: 7.39 seconds\n",
      "inferencing test set\n",
      "inference time: 7.17 seconds\n",
      "training\n",
      "training time: 31.48 seconds\n",
      "inferencing train set\n",
      "inference time: 4.45 seconds\n",
      "inferencing test set\n",
      "inference time: 10.67 seconds\n",
      "CPU times: user 3min 52s, sys: 31.8 s, total: 4min 24s\n",
      "Wall time: 2min 34s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for i in ['0.75_0.25', '0.5_0.5', '0.25_0.75']:\n",
    "    train, test = splits['train_' + i], splits['test_' + i]\n",
    "    res = evaluate(train, test, evaluators, cf)\n",
    "    result.append(res)\n",
    "ub_CF_res = pd.DataFrame(index=['rmse', 'accuracy', 'coverage_5'], columns = ['train', 'test', 'split'])\n",
    "for i, j in zip(result, ['0.75_0.25', '0.5_0.5', '0.25_0.75']):\n",
    "    i['split'] = j\n",
    "    ub_CF_res = ub_CF_res.append(i)\n",
    "ub_CF_res = ub_CF_res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.821108</td>\n",
       "      <td>0.894580</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.842982</td>\n",
       "      <td>0.803747</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.983591</td>\n",
       "      <td>0.708690</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.808453</td>\n",
       "      <td>0.921639</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.851055</td>\n",
       "      <td>0.797801</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.969546</td>\n",
       "      <td>0.945709</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.784599</td>\n",
       "      <td>0.980886</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.861927</td>\n",
       "      <td>0.779768</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.910176</td>\n",
       "      <td>0.967936</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train      test      split\n",
       "rmse        0.821108  0.894580  0.75_0.25\n",
       "accuracy    0.842982  0.803747  0.75_0.25\n",
       "coverage_2  0.983591  0.708690  0.75_0.25\n",
       "rmse        0.808453  0.921639    0.5_0.5\n",
       "accuracy    0.851055  0.797801    0.5_0.5\n",
       "coverage_2  0.969546  0.945709    0.5_0.5\n",
       "rmse        0.784599  0.980886  0.25_0.75\n",
       "accuracy    0.861927  0.779768  0.25_0.75\n",
       "coverage_2  0.910176  0.967936  0.25_0.75"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ub_CF_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub_CF_res.to_csv('../data/processed/ub_cf_res.csv', header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Memory_based_CF(spark, 'item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "training time: 6.04 seconds\n",
      "inferencing train set\n",
      "inference time: 11.87 seconds\n",
      "inferencing test set\n",
      "inference time: 3.95 seconds\n",
      "training\n",
      "training time: 5.83 seconds\n",
      "inferencing train set\n",
      "inference time: 7.73 seconds\n",
      "inferencing test set\n",
      "inference time: 7.5 seconds\n",
      "training\n",
      "training time: 4.75 seconds\n",
      "inferencing train set\n",
      "inference time: 4.16 seconds\n",
      "inferencing test set\n",
      "inference time: 10.59 seconds\n",
      "CPU times: user 56.4 s, sys: 2.1 s, total: 58.5 s\n",
      "Wall time: 1min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for i in ['0.75_0.25', '0.5_0.5', '0.25_0.75']:\n",
    "    train, test = splits['train_' + i], splits['test_' + i]\n",
    "    res = evaluate(train, test, evaluators, cf)\n",
    "    result.append(res)\n",
    "ib_CF_res = pd.DataFrame(index=['rmse', 'accuracy', 'coverage_5'], columns = ['train', 'test', 'split'])\n",
    "for i, j in zip(result, ['0.75_0.25', '0.5_0.5', '0.25_0.75']):\n",
    "    i['split'] = j\n",
    "    ib_CF_res = ib_CF_res.append(i)\n",
    "ib_CF_res = ib_CF_res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.847614</td>\n",
       "      <td>0.868404</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.837997</td>\n",
       "      <td>0.811680</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.977120</td>\n",
       "      <td>0.705143</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.867596</td>\n",
       "      <td>0.895520</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.837147</td>\n",
       "      <td>0.807913</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.951262</td>\n",
       "      <td>0.932456</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.942725</td>\n",
       "      <td>0.960093</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.825541</td>\n",
       "      <td>0.793047</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.864011</td>\n",
       "      <td>0.958940</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train      test      split\n",
       "rmse        0.847614  0.868404  0.75_0.25\n",
       "accuracy    0.837997  0.811680  0.75_0.25\n",
       "coverage_2  0.977120  0.705143  0.75_0.25\n",
       "rmse        0.867596  0.895520    0.5_0.5\n",
       "accuracy    0.837147  0.807913    0.5_0.5\n",
       "coverage_2  0.951262  0.932456    0.5_0.5\n",
       "rmse        0.942725  0.960093  0.25_0.75\n",
       "accuracy    0.825541  0.793047  0.25_0.75\n",
       "coverage_2  0.864011  0.958940  0.25_0.75"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ib_CF_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib_CF_res.to_csv('../data/processed/ib_cf_res.csv', header = True, index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ALS Matrix Factorization Approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import Cross_validate_als"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Parameter Tuning for ALS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"regParam\": [0.01, 0.05, 0.1, 0.15],\n",
    "    \"rank\": [10, 50, 100, 150, 200]\n",
    "}\n",
    "\n",
    "cf = cf = Als(userCol='userId',\n",
    "                itemCol='movieId',\n",
    "                ratingCol='rating',\n",
    "                regParam=.15,\n",
    "                seed=0,\n",
    "                rank=10\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [13:31<00:00, 40.59s/it]\n",
      "100%|██████████| 20/20 [12:29<00:00, 37.48s/it]\n",
      "100%|██████████| 20/20 [09:07<00:00, 27.37s/it]\n"
     ]
    }
   ],
   "source": [
    "result = []\n",
    "for i in ['0.75_0.25', '0.5_0.5', '0.25_0.75']:\n",
    "    train, test = splits['train_' + i], splits['test_' + i]\n",
    "    res = Cross_validate_als(training = train,\n",
    "                            test= test,\n",
    "                            valid_ratio = .1,\n",
    "                            regParam = parameters['regParam'],\n",
    "                            rank = parameters['rank'],\n",
    "                            seed = 0,\n",
    "                            evaluators = list(evaluators.values()))\n",
    "    result.append(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_parameter_tuning = pd.DataFrame(columns = ['rmse', 'accuracy', 'converage_k'])\n",
    "for i, j in zip(result, ['0.75_0.25', '0.5_0.5', '0.25_0.75']):\n",
    "    i['split'] = j\n",
    "    als_parameter_tuning = als_parameter_tuning.append(i)\n",
    "als_parameter_tuning = als_parameter_tuning.dropna()\n",
    "als_parameter_tuning.index.name = '(regParam, rank)'\n",
    "als_parameter_tuning.to_csv('../data/processed/als_parameter_tuning.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the best parameter for each split and metric is\n",
      "best param for accuracy\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "split\n",
       "0.25_0.75    (0.15, 10)\n",
       "0.5_0.5      (0.15, 10)\n",
       "0.75_0.25    (0.15, 10)\n",
       "Name: rmse, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best param for rmse\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "split\n",
       "0.25_0.75    (0.15, 10)\n",
       "0.5_0.5      (0.15, 10)\n",
       "0.75_0.25    (0.15, 10)\n",
       "Name: accuracy, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best param for coverage_k\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "split\n",
       "0.25_0.75    (0.15, 10)\n",
       "0.5_0.5      (0.15, 10)\n",
       "0.75_0.25    (0.15, 10)\n",
       "Name: accuracy, dtype: object"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f'the best parameter for each split and metric is')\n",
    "print(f'best param for accuracy')\n",
    "display(als_parameter_tuning.groupby('split').rmse.apply(lambda x: x.idxmin()))\n",
    "print(f'best param for rmse')\n",
    "display(als_parameter_tuning.groupby('split').accuracy.apply(lambda x: x.idxmax()))\n",
    "print(f'best param for coverage_k')\n",
    "display(als_parameter_tuning.groupby('split').accuracy.apply(lambda x: x.idxmax()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will choose the best parameter regParam = .15, rank = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ALS evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Als(userCol='userId',\n",
    "                itemCol='movieId',\n",
    "                ratingCol='rating',\n",
    "                regParam=.15,\n",
    "                seed=0,\n",
    "                rank=10\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "training time: 3.74 seconds\n",
      "inferencing train set\n",
      "inference time: 0.02 seconds\n",
      "inferencing test set\n",
      "inference time: 0.02 seconds\n",
      "training\n",
      "training time: 2.01 seconds\n",
      "inferencing train set\n",
      "inference time: 0.02 seconds\n",
      "inferencing test set\n",
      "inference time: 0.02 seconds\n",
      "training\n",
      "training time: 1.57 seconds\n",
      "inferencing train set\n",
      "inference time: 0.02 seconds\n",
      "inferencing test set\n",
      "inference time: 0.01 seconds\n",
      "CPU times: user 97.2 ms, sys: 33.1 ms, total: 130 ms\n",
      "Wall time: 26.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for i in ['0.75_0.25', '0.5_0.5', '0.25_0.75']:\n",
    "    train, test = splits['train_' + i], splits['test_' + i]\n",
    "    res = evaluate(train, test, evaluators, cf)\n",
    "    result.append(res)\n",
    "als_cf_res = pd.DataFrame(index=['rmse', 'accuracy', 'coverage_5'], columns = ['train', 'test', 'split'])\n",
    "for i, j in zip(result, ['0.75_0.25', '0.5_0.5', '0.25_0.75']):\n",
    "    i['split'] = j\n",
    "    als_cf_res = als_cf_res.append(i)\n",
    "als_cf_res = als_cf_res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "als_cf_res.to_csv('../data/processed/als_result.csv', header=True, index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.706560</td>\n",
       "      <td>0.876594</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.845688</td>\n",
       "      <td>0.768591</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.975542</td>\n",
       "      <td>0.692619</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.649651</td>\n",
       "      <td>0.915222</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.859147</td>\n",
       "      <td>0.751764</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.945363</td>\n",
       "      <td>0.903569</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.540238</td>\n",
       "      <td>1.009128</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.872770</td>\n",
       "      <td>0.705264</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.845193</td>\n",
       "      <td>0.923880</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train      test      split\n",
       "rmse        0.706560  0.876594  0.75_0.25\n",
       "accuracy    0.845688  0.768591  0.75_0.25\n",
       "coverage_2  0.975542  0.692619  0.75_0.25\n",
       "rmse        0.649651  0.915222    0.5_0.5\n",
       "accuracy    0.859147  0.751764    0.5_0.5\n",
       "coverage_2  0.945363  0.903569    0.5_0.5\n",
       "rmse        0.540238  1.009128  0.25_0.75\n",
       "accuracy    0.872770  0.705264  0.25_0.75\n",
       "coverage_2  0.845193  0.923880  0.25_0.75"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "als_cf_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalization",
   "language": "python",
   "name": "personalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
