{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **IEOR 4571 Fall 2020 Homework2 Report**\n",
    "\n",
    "\n",
    "- Hu, Bo (uni: bh2569)\n",
    "- Qin, Rui (uni: rq217)\n",
    "- Yuan, Shuibenyang (uni: sy2938)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# todo how did we sample the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils import loading, Spark\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI address http://127.0.0.1:4041\n"
     ]
    }
   ],
   "source": [
    "# create spark session\n",
    "spark = Spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load sample data from '/data/raw/sample.csv'\n",
    "datas = loading(spark, '../data/raw')\n",
    "sample = datas['sample']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Objectives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "            number of data points in the sample: 444289,\n",
      "            number of unique users in the sample: 20000,\n",
      "            number of unique movies in the sample: 1000,\n",
      "            mean of number of movies a user rated:22.21445,\n",
      "            mean of number of users a movie be rated: 444.289,\n",
      "            average rating: 3.562069958968149,\n",
      "            standard deviation of rating: 1.046784241962096,\n",
      "            average rating by user: 3.685826601448731,\n",
      "            standard deviation of rating by user mean: 0.5225862364055156,\n",
      "            average rating by movie: 3.290922029378176,\n",
      "            standard deviation of rating by movie mean: 0.5133362661920545\n",
      "        \n"
     ]
    }
   ],
   "source": [
    "print(f'''\n",
    "            number of data points in the sample: {sample.count()},\n",
    "            number of unique users in the sample: {sample.select('userId').distinct().count()},\n",
    "            number of unique movies in the sample: {sample.select('movieId').distinct().count()},\n",
    "            mean of number of movies a user rated:{sample.groupby('userId').agg(F.count('movieId').alias('cnt')).select(F.mean('cnt')).collect()[0][0]},\n",
    "            mean of number of users a movie be rated: {sample.groupby('movieId').agg(F.count('userId').alias('cnt')).select(F.mean('cnt')).collect()[0][0]},\n",
    "            average rating: {sample.select(F.mean('rating')).collect()[0][0]},\n",
    "            standard deviation of rating: {sample.select(F.stddev('rating')).collect()[0][0]},\n",
    "            average rating by user: {sample.groupby('userId').agg(F.mean('rating').alias('rating')).select(F.mean('rating')).collect()[0][0]},\n",
    "            standard deviation of rating by user mean: {sample.groupby('userId').agg(F.mean('rating').alias('rating')).select(F.stddev('rating')).collect()[0][0]},\n",
    "            average rating by movie: {sample.groupby('movieId').agg(F.mean('rating').alias('rating')).select(F.mean('rating')).collect()[0][0]},\n",
    "            standard deviation of rating by movie mean: {sample.groupby('movieId').agg(F.mean('rating').alias('rating')).select(F.stddev('rating')).collect()[0][0]}\n",
    "        ''')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Validation Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluation import Evaluator, Cross_validate_als\n",
    "from src.model_based import Als\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = loading(spark, '../data/interim')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_0.5_0.5', 'train_0.25_0.75', 'test_0.5_0.5', 'test_0.25_0.75', 'train_0.75_0.25', 'test_0.75_0.25']\n"
     ]
    }
   ],
   "source": [
    "print(list(splits.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build evaluation pipeline\n",
    "def evaluate(train, test, evaluators, model):\n",
    "    print('training')\n",
    "    start = time.time()\n",
    "    model.fit(train)\n",
    "    end = time.time()\n",
    "    print(f'training time: {round(end - start, 2)} seconds')\n",
    "    print('inferencing train set')\n",
    "    start = time.time()\n",
    "    train_pred = model.predict(train)\n",
    "    end = time.time()\n",
    "    print(f'inference time: {round(end - start, 2)} seconds')\n",
    "    print('inferencing test set')\n",
    "    start = time.time()\n",
    "    test_pred = model.predict(test)\n",
    "    end = time.time()\n",
    "    print(f'inference time: {round(end - start, 2)} seconds')\n",
    "    res = pd.DataFrame(np.zeros((len(evaluators),2)), columns = ['train', 'test'], index = evaluators.keys())\n",
    "    for eva in evaluators.keys():\n",
    "        res.loc[eva, 'train'] = evaluators[eva].evaluate(train_pred)\n",
    "        res.loc[eva, 'test'] = evaluators[eva].evaluate(test_pred)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {'rmse': Evaluator(metrics = 'rmse'), \n",
    "              'accuracy': Evaluator(metrics = 'accuracy'), \n",
    "              'coverage_2': Evaluator(metrics = 'converage_k', \n",
    "                                       ratingCol='rating', \n",
    "                                       predCol='prediction', \n",
    "                                       idCol='userId', \n",
    "                                       k=2)}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.memory_based import Memory_based_CF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Memory_based_CF(spark, 'user')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training\n",
      "training time: 38.8 seconds\n",
      "inferencing train set\n",
      "inference time: 10.88 seconds\n",
      "inferencing test set\n",
      "inference time: 4.08 seconds\n",
      "training\n",
      "training time: 36.11 seconds\n",
      "inferencing train set\n",
      "inference time: 7.5 seconds\n",
      "inferencing test set\n",
      "inference time: 7.62 seconds\n",
      "training\n",
      "training time: 28.85 seconds\n",
      "inferencing train set\n",
      "inference time: 3.83 seconds\n",
      "inferencing test set\n",
      "inference time: 10.68 seconds\n",
      "CPU times: user 3min 53s, sys: 32.7 s, total: 4min 25s\n",
      "Wall time: 2min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "result = []\n",
    "for i in ['0.75_0.25', '0.5_0.5', '0.25_0.75']:\n",
    "    train, test = splits['train_' + i], splits['test_' + i]\n",
    "    res = evaluate(train, test, evaluators, cf)\n",
    "    result.append(res)\n",
    "ub_CF_res = pd.DataFrame(index=['rmse', 'accuracy', 'coverage_5'], columns = ['train', 'test', 'split'])\n",
    "for i, j in zip(result, ['0.75_0.25', '0.5_0.5', '0.25_0.75']):\n",
    "    i['split'] = j\n",
    "    ub_CF_res = ub_CF_res.append(i)\n",
    "ub_CF_res = ub_CF_res.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train</th>\n",
       "      <th>test</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.822981</td>\n",
       "      <td>0.882472</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.737826</td>\n",
       "      <td>0.705916</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.931907</td>\n",
       "      <td>0.707219</td>\n",
       "      <td>0.75_0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.812562</td>\n",
       "      <td>0.903506</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.744819</td>\n",
       "      <td>0.695154</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.880891</td>\n",
       "      <td>0.861922</td>\n",
       "      <td>0.5_0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>rmse</th>\n",
       "      <td>0.789096</td>\n",
       "      <td>0.961355</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>accuracy</th>\n",
       "      <td>0.761302</td>\n",
       "      <td>0.674534</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>coverage_2</th>\n",
       "      <td>0.734384</td>\n",
       "      <td>0.920091</td>\n",
       "      <td>0.25_0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               train      test      split\n",
       "rmse        0.822981  0.882472  0.75_0.25\n",
       "accuracy    0.737826  0.705916  0.75_0.25\n",
       "coverage_2  0.931907  0.707219  0.75_0.25\n",
       "rmse        0.812562  0.903506    0.5_0.5\n",
       "accuracy    0.744819  0.695154    0.5_0.5\n",
       "coverage_2  0.880891  0.861922    0.5_0.5\n",
       "rmse        0.789096  0.961355  0.25_0.75\n",
       "accuracy    0.761302  0.674534  0.25_0.75\n",
       "coverage_2  0.734384  0.920091  0.25_0.75"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ub_CF_res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Item Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Memory_based_CF(spark, 'item')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "result = []\n",
    "for i in ['0.75_0.25', '0.5_0.5', '0.25_0.75']:\n",
    "    train, test = splits['train_' + i], splits['test_' + i]\n",
    "    res = evaluate(train, test, evaluators, cf)\n",
    "    result.append(res)\n",
    "ib_CF_res = pd.DataFrame(index=['rmse', 'accuracy', 'coverage_5'], columns = ['train', 'test', 'split'])\n",
    "for i, j in zip(result, ['0.75_0.25', '0.5_0.5', '0.25_0.75']):\n",
    "    i['split'] = j\n",
    "    ib_CF_res = ib_CF_res.append(i)\n",
    "ib_CF_res = ib_CF_res.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Based Collaborative Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalization",
   "language": "python",
   "name": "personalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
