{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Execution Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI address http://127.0.0.1:4040\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2 \n",
    "import sys\n",
    "sys.path.append('../')\n",
    "\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.types as T\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "from src.utils import loading, Spark\n",
    "from src.evaluation import Evaluator, Cross_validate_als\n",
    "from src.model_based import Als\n",
    "from src.baseline import Baseline\n",
    "from src.memory_based import Memory_based_CF\n",
    "from src.model_based import Als\n",
    "\n",
    "# create spark session\n",
    "spark = Spark()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Splitted Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train_0.5_0.5', 'train_0.25_0.75', 'test_0.5_0.5', 'test_0.25_0.75', 'train_0.75_0.25', 'test_0.75_0.25']\n"
     ]
    }
   ],
   "source": [
    "data = loading(spark, '../data/interim')\n",
    "splits = ['0.75_0.25', '0.5_0.5', '0.25_0.75']\n",
    "print(list(data.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Evaluators (Metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluators = {'rmse': Evaluator(metrics = 'rmse'), \n",
    "              'accuracy': Evaluator(metrics = 'accuracy'), \n",
    "              'coverage_user': Evaluator(metrics = 'converage_k', \n",
    "                                       ratingCol='rating', \n",
    "                                       predCol='prediction', \n",
    "                                       idCol='userId', \n",
    "                                       k=10),\n",
    "              'coverage_item': Evaluator(metrics = 'converage_k', \n",
    "                                       ratingCol='rating', \n",
    "                                       predCol='prediction', \n",
    "                                       idCol='movieId', \n",
    "                                       k=100)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build evaluation pipeline\n",
    "def evaluate(train, test, evaluators, model):\n",
    "    start = time.time()\n",
    "    model.fit(train)\n",
    "    training_time = time.time() - start\n",
    "    start = time.time()\n",
    "    train_pred = model.predict(train)\n",
    "    inference_train = time.time() - start\n",
    "    start = time.time()\n",
    "    test_pred = model.predict(test)\n",
    "    inference_test = time.time() - start\n",
    "    res = pd.DataFrame(np.zeros((len(evaluators),2)), columns = ['train', 'test'], index = evaluators.keys())\n",
    "    for eva in evaluators.keys():\n",
    "        res.loc[eva, 'train'] = evaluators[eva].evaluate(train_pred)\n",
    "        res.loc[eva, 'test'] = evaluators[eva].evaluate(test_pred)\n",
    "    return res, pd.Series({'training time': training_time, \n",
    "                           'inference train time':inference_train, \n",
    "                           'inference test time':inference_test})\n",
    "\n",
    "def evaluate_pipeline(data, splits, model):\n",
    "    result = []\n",
    "    time = pd.DataFrame(columns = splits)\n",
    "    for i in splits:\n",
    "        train, test = data['train_' + i], data['test_' + i]\n",
    "        res, time[i] = evaluate(train, test, evaluators, model)\n",
    "        result.append(res)\n",
    "    res = pd.DataFrame(columns = ['train', 'test', 'split'])\n",
    "    for i, j in zip(result, splits):\n",
    "        i['split'] = j\n",
    "        res = res.append(i)\n",
    "    return res, time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Declaring Estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline = Baseline(usercol='userId', itemcol='movieId', ratingcol='rating')\n",
    "userbased = Memory_based_CF(spark, base='user', usercol='userId', itemcol='movieId', ratingcol='rating')\n",
    "itembased = Memory_based_CF(spark, base='item', usercol='userId', itemcol='movieId', ratingcol='rating')\n",
    "modelbased = Als(userCol='userId', itemCol='movieId', ratingCol='rating', regParam=.15, seed=0, rank=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runing Evaluation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 119 ms, sys: 42 ms, total: 161 ms\n",
      "Wall time: 31.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "baseline_res, baseline_time_res = evaluate_pipeline(data, splits, baseline)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 5s, sys: 37.1 s, total: 4min 43s\n",
      "Wall time: 2min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "userbased_res, userbased_time_res = evaluate_pipeline(data, splits, userbased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 58.2 s, sys: 2.56 s, total: 1min\n",
      "Wall time: 1min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "itembased_res, itembased_time_res = evaluate_pipeline(data, splits, itembased)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 132 ms, sys: 46.3 ms, total: 178 ms\n",
      "Wall time: 39.4 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "modelbased_res, modelbased_time_res = evaluate_pipeline(data, splits, modelbased)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "baseline_res.to_csv('../data/processed/baseline_res.csv', header = True, index = True)\n",
    "userbased_res.to_csv('../data/processed/userbased_res.csv', header = True, index = True)\n",
    "itembased_res.to_csv('../data/processed/itembased_res.csv', header = True, index = True)\n",
    "modelbased_res.to_csv('../data/processed/modelbased_res.csv', header = True, index = True)\n",
    "\n",
    "baseline_time_res.to_csv('../data/processed/baseline_time_res.csv', header = True, index = True)\n",
    "userbased_time_res.to_csv('../data/processed/userbased_time_res.csv', header = True, index = True)\n",
    "itembased_time_res.to_csv('../data/processed/itembased_time_res.csv', header = True, index = True)\n",
    "modelbased_time_res.to_csv('../data/processed/modelbased_time_res.csv', header = True, index = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "personalization",
   "language": "python",
   "name": "personalization"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
