{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "personalization",
      "language": "python",
      "name": "personalization"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    },
    "latex_envs": {
      "LaTeX_envs_menu_present": true,
      "autoclose": false,
      "autocomplete": true,
      "bibliofile": "biblio.bib",
      "cite_by": "apalike",
      "current_citInitial": 1,
      "eqLabelWithNumbers": true,
      "eqNumInitial": 1,
      "hotkeys": {
        "equation": "Ctrl-E",
        "itemize": "Ctrl-I"
      },
      "labels_anchors": false,
      "latex_user_defs": false,
      "report_style_numbering": false,
      "user_envs_cfg": false
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "varInspector": {
      "cols": {
        "lenName": 16,
        "lenType": 16,
        "lenVar": 40
      },
      "kernels_config": {
        "python": {
          "delete_cmd_postfix": "",
          "delete_cmd_prefix": "del ",
          "library": "var_list.py",
          "varRefreshCmd": "print(var_dic_list())"
        },
        "r": {
          "delete_cmd_postfix": ") ",
          "delete_cmd_prefix": "rm(",
          "library": "var_list.r",
          "varRefreshCmd": "cat(var_dic_list()) "
        }
      },
      "types_to_exclude": [
        "module",
        "function",
        "builtin_function_or_method",
        "instance",
        "_Feature"
      ],
      "window_display": false
    },
    "colab": {
      "name": "wide_n_deep_training.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ds-personalization/final-project-qrdecomposition_final/blob/main/notebook/wide_n_deep_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w4KvVL7bAoz6",
        "outputId": "219fc715-f42e-4f0c-d5e1-901883ba6a1e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GaAwbZEXB19R"
      },
      "source": [
        "import os\n",
        "data_path = os.path.join('/content/drive/', 'MyDrive','final-project-qrdecomposition_final','data')"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K2jZ9WS7BS8U"
      },
      "source": [
        "!pip install pytorch_widedeep --quiet"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M7IVlfT3Aoz_"
      },
      "source": [
        "import os\n",
        "###utilities\n",
        "from tqdm import tqdm\n",
        "import time\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "###numpy,scipy,pandas,sklearn stacks\n",
        "from scipy import sparse\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import FunctionTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "###torch stacks\n",
        "import torch\n",
        "from torch import nn\n",
        "from pytorch_widedeep.preprocessing import DensePreprocessor\n",
        "from pytorch_widedeep.callbacks import (\n",
        "    LRHistory,\n",
        "    EarlyStopping,\n",
        "    ModelCheckpoint,\n",
        ")\n",
        "from pytorch_widedeep.optim import RAdam\n",
        "from pytorch_widedeep.initializers import XavierNormal, KaimingNormal\n",
        "from pytorch_widedeep.models import Wide, DeepDense, WideDeep"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jB1Q-EZUAoz_"
      },
      "source": [
        "class wide_deep():\n",
        "    def __init__(self,wide_cols='genres',\n",
        "                    deep_cols=['userId', 'movieId'],\n",
        "                    target_col = 'rating',\n",
        "                    deep_embs=[64, 64],\n",
        "                    deep_hidden=[64,32,16],\n",
        "                    deep_dropout=[0.1, 0.1, .1],\n",
        "                    deep_bachnorm=True):\n",
        "        self.wide = None\n",
        "        self.deep = None\n",
        "        self.deep_hidden = deep_hidden\n",
        "        self.deep_dropout = deep_dropout\n",
        "        self.deep_bachnorm = deep_bachnorm\n",
        "        self.model = None\n",
        "\n",
        "        self.embs = [(col, dim) for col, dim in zip(deep_cols, deep_embs)]\n",
        "        self.wide_preprocessor = self._genre_preprocessor(wide_cols)\n",
        "        self.deep_preprocessor = DensePreprocessor(embed_cols=self.embs)\n",
        "        self.target_col = target_col\n",
        "\n",
        "\n",
        "    def fit(self, train, n_epochs=10, batch_size=128, val_split=.1, verbose = True):\n",
        "        X, y = train.drop(self.target_col, axis = 1), train[self.target_col].values\n",
        "        wide_feature = self.wide_preprocessor.fit_transform(X)\n",
        "        deep_feature = self.deep_preprocessor.fit_transform(X)\n",
        "        self.wide = Wide(wide_dim=np.unique(wide_feature).shape[0], pred_dim=1)\n",
        "        self.deep = DeepDense(hidden_layers=self.deep_hidden, dropout=self.deep_dropout,\n",
        "                      batchnorm=self.deep_bachnorm,\n",
        "                      deep_column_idx=self.deep_preprocessor.deep_column_idx,\n",
        "                      embed_input=self.deep_preprocessor.embeddings_input)\n",
        "        self.model =  WideDeep(wide=self.wide, deepdense=self.deep)\n",
        "        wide_opt = torch.optim.Adam(self.model.wide.parameters(), lr=0.01)\n",
        "        deep_opt = RAdam(self.model.deepdense.parameters())\n",
        "        wide_sch = torch.optim.lr_scheduler.StepLR(wide_opt, step_size=3)\n",
        "        deep_sch = torch.optim.lr_scheduler.StepLR(deep_opt, step_size=5)\n",
        "        callbacks = [\n",
        "                        LRHistory(n_epochs=n_epochs),\n",
        "                        EarlyStopping(patience=5),\n",
        "                        ModelCheckpoint(filepath=\"model_weights/wd_out\"),\n",
        "                    ]\n",
        "        optimizers = {\"wide\": wide_opt, \"deepdense\": deep_opt}\n",
        "        schedulers = {\"wide\": wide_sch, \"deepdense\": deep_sch}\n",
        "        initializers = {\"wide\": KaimingNormal, \"deepdense\": XavierNormal}\n",
        "        self.model.compile(method='regression',\n",
        "                            optimizers=optimizers,\n",
        "                        lr_schedulers=schedulers,\n",
        "                        initializers=initializers,\n",
        "                        callbacks=callbacks,\n",
        "                        verbose=verbose)\n",
        "        self.model.fit(X_wide=wide_feature, \n",
        "                  X_deep=deep_feature, \n",
        "                  target=y, \n",
        "                  n_epochs=n_epochs, \n",
        "                  batch_size=batch_size, \n",
        "                  val_split=val_split,)\n",
        "        \n",
        "    def predict(self, test):\n",
        "        X, y = test.drop(self.target_col, axis = 1), test[self.target_col].values\n",
        "        wide_feature = self.wide_preprocessor.transform(X)\n",
        "        deep_feature = self.deep_preprocessor.transform(X)\n",
        "        return self.model.predict(X_wide=wide_feature, X_deep=deep_feature)\n",
        "\n",
        "    def _genre_preprocessor(self, genre_feat):\n",
        "        dense_layer = lambda X: X.toarray()\n",
        "        genre_transformer = Pipeline(steps=[\n",
        "                ('tokenizer', CountVectorizer()),\n",
        "                ('dense', FunctionTransformer(dense_layer, validate=False))   \n",
        "        ])\n",
        "        preproc = ColumnTransformer(transformers=[('genre', genre_transformer, genre_feat),])\n",
        "        return preproc\n",
        "\n",
        "\n",
        "    def _deep_preprocessor(self,embs):\n",
        "        return DensePreprocessor(embed_cols=embs)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ebdcwoxpAo0A"
      },
      "source": [
        "sample_train_df, sample_test_df = pd.read_csv(os.path.join(data_path, 'sample_train.csv')), \\\n",
        "                                    pd.read_csv(os.path.join(data_path, 'sample_test.csv'))\n",
        "movies = pd.read_csv(os.path.join(data_path, 'movies.csv'))\n",
        "sample_train_df, sample_test_df = sample_train_df.merge(movies), sample_test_df.merge(movies)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Uy9-7B-JAo0A"
      },
      "source": [
        "wd = wide_deep()"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tDf24w-xAo0A",
        "outputId": "a6746e87-c3ab-4c8c-b13b-0a90d6f834f4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "wd.fit(sample_train_df)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r  0%|          | 0/23001 [00:00<?, ?it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Training\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "epoch 1: 100%|██████████| 23001/23001 [07:17<00:00, 52.55it/s, loss=1.21]\n",
            "valid: 100%|██████████| 2556/2556 [00:27<00:00, 92.99it/s, loss=0.68]\n",
            "epoch 2: 100%|██████████| 23001/23001 [07:11<00:00, 53.27it/s, loss=0.675]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 94.86it/s, loss=0.662]\n",
            "epoch 3: 100%|██████████| 23001/23001 [07:06<00:00, 53.87it/s, loss=0.644]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 95.94it/s, loss=0.66]\n",
            "epoch 4: 100%|██████████| 23001/23001 [07:04<00:00, 54.23it/s, loss=0.616]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 95.34it/s, loss=0.662]\n",
            "epoch 5: 100%|██████████| 23001/23001 [07:04<00:00, 54.16it/s, loss=0.595]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 95.22it/s, loss=0.661]\n",
            "epoch 6: 100%|██████████| 23001/23001 [07:08<00:00, 53.72it/s, loss=0.593]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 95.39it/s, loss=0.638]\n",
            "epoch 7: 100%|██████████| 23001/23001 [07:07<00:00, 53.86it/s, loss=0.581]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 95.16it/s, loss=0.634] \n",
            "epoch 8: 100%|██████████| 23001/23001 [07:09<00:00, 53.52it/s, loss=0.576]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 95.39it/s, loss=0.632]\n",
            "epoch 9: 100%|██████████| 23001/23001 [07:10<00:00, 53.40it/s, loss=0.571]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 95.88it/s, loss=0.631]\n",
            "epoch 10: 100%|██████████| 23001/23001 [07:09<00:00, 53.54it/s, loss=0.567]\n",
            "valid: 100%|██████████| 2556/2556 [00:26<00:00, 96.42it/s, loss=0.631]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NXZG6HTgTveK",
        "outputId": "03532ec3-74c6-4c43-8e51-e7b03099aa66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "train_pred = wd.predict(sample_train_df)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict: 100%|██████████| 25557/25557 [03:02<00:00, 140.23it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIJgtwGnTorg",
        "outputId": "607d492c-b40b-4eaf-b0f1-8c6253704847",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "test_pred = wd.predict(sample_test_df)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "predict: 100%|██████████| 8510/8510 [01:00<00:00, 139.84it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AakSrV7QAo0B"
      },
      "source": [
        "torch.save(wd.model, os.path.join(data_path, \"wide_deep_sample.t\"))"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lWQMKrA-ZwOk"
      },
      "source": [
        "with open(os.path.join(data_path, \"wide_deep_sample_train_pred.npy\"), 'wb') as f:\n",
        "    np.save(f, train_pred)"
      ],
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nNFhaDwaP4t"
      },
      "source": [
        "with open(os.path.join(data_path, \"wide_deep_sample_test_pred.npy\"), 'wb') as f:\n",
        "    np.save(f, test_pred)"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iPclZNJSZgeB"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}