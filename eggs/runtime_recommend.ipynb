{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "repo_name = 'final-project-qrdecomposition_final'\n",
    "data_path = '../downloads'\n",
    "\n",
    "if not os.path.isdir(data_path):\n",
    "    os.mkdir(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda 1.6.0+cu101\n"
     ]
    }
   ],
   "source": [
    "###utilities\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "###pyspark dependencies\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.ml as M\n",
    "import pyspark.sql.functions as F\n",
    "import pyspark.sql.window as W\n",
    "import pyspark.sql.types as T\n",
    "from pyspark.ml.recommendation import ALS\n",
    "\n",
    "###numpy,scipy,pandas,sklearn stacks\n",
    "from scipy import sparse\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "###torch stacks\n",
    "import torch\n",
    "from torch import nn\n",
    "from pytorch_widedeep.preprocessing import DensePreprocessor\n",
    "from pytorch_widedeep.callbacks import (\n",
    "    LRHistory,\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint,\n",
    ")\n",
    "from pytorch_widedeep.optim import RAdam\n",
    "from pytorch_widedeep.initializers import XavierNormal, KaimingNormal\n",
    "from pytorch_widedeep.models import Wide, DeepDense, WideDeep\n",
    "use_cuda = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "print(device, torch.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark UI address http://127.0.0.1:4040\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"JAVA_HOME\"] = \"/datasets/home/65/965/yux164/.jdk/jdk-11.0.9.1+1\" #for java path\n",
    "import psutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SparkContext, SparkConf\n",
    "NUM_WORKER = psutil.cpu_count(logical = False)\n",
    "NUM_THREAD = psutil.cpu_count(logical = True)\n",
    "def spark_session():\n",
    "    \"\"\"[function for creating spark session]\n",
    "\n",
    "    Returns:\n",
    "        [Spark Session]: [the spark session]\n",
    "    \"\"\"    \n",
    "    conf_spark = SparkConf().set(\"spark.driver.host\", \"127.0.0.1\")\\\n",
    "                            .set(\"spark.executor.instances\", NUM_WORKER)\\\n",
    "                            .set(\"spark.executor.cores\", int(NUM_THREAD / NUM_WORKER))\\\n",
    "                            .set(\"spark.executor.memory\", '4g')\\\n",
    "                            .set(\"spark.sql.shuffle.partitions\", NUM_THREAD)\n",
    "    sc = SparkContext(conf = conf_spark)\n",
    "    sc.setLogLevel('ERROR')\n",
    "    spark = SparkSession(sc)\n",
    "    print('Spark UI address {}'.format(spark.sparkContext.uiWebUrl))\n",
    "    return spark\n",
    "\n",
    "spark = spark_session()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples/\n",
      "samples/sample_train.csv\n",
      "samples/sample.csv\n",
      "samples/sample_test.csv\n",
      "model_results  movies.csv  samples\n"
     ]
    }
   ],
   "source": [
    "#load sample from local path\n",
    "compressed_sample_path = '../data/sample.tar.gz'\n",
    "!tar -xzvf $compressed_sample_path -C $data_path\n",
    "!ls $data_path\n",
    "\n",
    "sample_path = os.path.join(data_path, 'samples', 'sample.csv')\n",
    "sample = spark.read.csv(sample_path, header=True).select('userId', 'movieId', 'rating').persist()\n",
    "sample_df = pd.read_csv(sample_path).drop('timestamp', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load from local files\n",
    "sample_train_path = os.path.join(data_path, 'samples', 'sample_train.csv')\n",
    "sample_test_path = os.path.join(data_path, 'samples', 'sample_test.csv')\n",
    "movie_path =  os.path.join(data_path, 'movies.csv')\n",
    "\n",
    "sample_train = spark.read.csv(sample_train_path, header=True)\n",
    "sample_test = spark.read.csv(sample_test_path, header=True)\n",
    "sample_train_df = pd.read_csv(sample_train_path)\n",
    "sample_test_df = pd.read_csv(sample_test_path)\n",
    "\n",
    "movies = spark.read.csv(movie_path, header=True)\n",
    "movies_df = pd.read_csv(movie_path)\n",
    "\n",
    "sample_df = sample_df.merge(movies_df)\n",
    "sample_train_df, sample_test_df = sample_train_df.merge(movies_df), sample_test_df.merge(movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4340404 3255932 1084472\n",
      "(4340404, 5) (3255932, 5) (1084472, 5)\n"
     ]
    }
   ],
   "source": [
    "print(sample.count(), sample_train.count(), sample_test.count())\n",
    "\n",
    "print(sample_df.shape, sample_train_df.shape, sample_test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def base_recommend(spark, \n",
    "                   base_model, \n",
    "                   cold_start_model, \n",
    "                   user_ids, \n",
    "                   movies, \n",
    "                   n,\n",
    "                   extra_features,\n",
    "                   user_id, \n",
    "                   item_id):\n",
    "    \n",
    "    userset = list(set(user_ids))\n",
    "    users = spark.createDataFrame(pd.DataFrame({base_model.userCol: userset}))\n",
    "    base_recommend = base_model.recommend(users, n).toPandas()\n",
    "    base_recommend = base_recommend.merge(movies, how='left')\n",
    "    base_recommend = base_recommend[[user_id, item_id] + extra_features]\n",
    "    base_recommend = base_recommend.astype({user_id: np.int64,\n",
    "                                            item_id: np.int64})\n",
    "    cold_start_users = set(user_ids) - set(base_recommend[user_id].tolist())\n",
    "    for user in cold_start_users:\n",
    "        cold_recommend = cold_start_model.recommend().toPandas().values.reshape(-1,)\n",
    "        user_lst = [user for _ in range(n)]\n",
    "        cold_recommendation = pd.DataFrame({user_id: user_lst, item_id: cold_recommend})\n",
    "        cold_recommendation = cold_recommendation.astype({user_id: np.int64,\n",
    "                                                        item_id: np.int64})\n",
    "        cold_recommendation = cold_recommendation.merge(movies, how='left')\n",
    "        cold_recommendation = cold_recommendation[[user_id, item_id] + extra_features]\n",
    "        base_recommend = base_recommend.append(cold_recommendation, ignore_index=True)\n",
    "    return base_recommend\n",
    "def advanced_recommend(advanced_recommender,\n",
    "                       base_recommend, \n",
    "                       k, \n",
    "                       user_id, \n",
    "                       item_id):\n",
    "    df = base_recommend.copy()\n",
    "    prediction = advanced_model.predict(df)\n",
    "    df['prediction'] = prediction\n",
    "    df = df.set_index(item_id).groupby(user_id).prediction\\\n",
    "        .apply(lambda x: x.sort_values(ascending=False)[:k]).reset_index()\n",
    "    return df\n",
    "\n",
    "def final_recommender(spark, \n",
    "                base_model, \n",
    "                cold_start_model, \n",
    "                advanced_recommender,\n",
    "                users,\n",
    "                movies,\n",
    "                n = 50,\n",
    "                k = 5,\n",
    "                user_id = 'userId',\n",
    "                item_id = 'movieId',\n",
    "                extra_features = ['genres']\n",
    "               ):\n",
    "    base_recommend_items  = base_recommend(spark, base_model, cold_start_model, users, movies, n, extra_features, user_id, item_id)\n",
    "    return advanced_recommend(advanced_recommender, base_recommend_items, k, user_id, item_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Memory_based_CF():\n",
    "    def __init__(self, spark, base, usercol='userId', itemcol='movieId', ratingcol='rating'):\n",
    "        \"\"\"[the memory based collabritive filtering model]\n",
    "        Args:\n",
    "            spark (Spark Session): [the current spark session]\n",
    "            base (str): [user base or item base]\n",
    "            usercol (str, optional): [user column name]. Defaults to 'userId'.\n",
    "            itemcol (str, optional): [item column name]. Defaults to 'movieId'.\n",
    "            ratingcol (str, optional): [rating/target column name]. Defaults to 'rating'.\n",
    "        \"\"\"        \n",
    "        self.base = base\n",
    "        self.usercol = usercol\n",
    "        self.itemcol = itemcol\n",
    "        self.ratingcol = ratingcol\n",
    "        self.spark = spark\n",
    "        self.X = None\n",
    "        self.idxer = None\n",
    "        self.similarity_matrix = None\n",
    "        self.prediction_matrix = None\n",
    "    def fit(self, _X):\n",
    "        \"\"\"[to train the model]\n",
    "        Args:\n",
    "            _X (Pyspark DataFrame): [the training set]\n",
    "        \"\"\"\n",
    "        X = self._preprocess(_X, True)\n",
    "        self.X = X\n",
    "        self.similarity_matrix = self._pearson_corr(X)\n",
    "        self.prediction_matrix = self._get_predict()\n",
    "        \n",
    "    def predict(self, _X):\n",
    "        \"\"\"[to predict based on trained model]\n",
    "        Args:\n",
    "            _X (Pyspark DataFrame): [the DataFrame needed to make prediction]\n",
    "        Returns:\n",
    "            [Pyspark DataFrame]: [the DataFrame with prediction column]\n",
    "        \"\"\"        \n",
    "        rows, cols = self._preprocess(_X, False)\n",
    "        preds = []\n",
    "        for i,j in zip(rows,cols):   \n",
    "            preds.append(self.prediction_matrix[i, j])\n",
    "        df = self.idxer.transform(_X).select(self.usercol, self.itemcol, self.ratingcol).toPandas()\n",
    "        df['prediction'] = preds\n",
    "        return self.spark.createDataFrame(df)\n",
    "\n",
    "    def recommend(self, X, numItem):\n",
    "        idices = self.idxer.u_indxer.transform(X).toPandas()['userId_idx'].values.astype(int)\n",
    "        items = np.asarray(np.argsort(self.prediction_matrix.T[idices, :])[:, -numItem:])\n",
    "        result = np.zeros((1, 3))\n",
    "        inverse_imat = pd.Series(self.idxer.i_indxer.labels)\n",
    "        inverse_umat = pd.Series(self.idxer.u_indxer.labels)\n",
    "        for u, i in zip(idices, items):\n",
    "            result = np.vstack((result, np.hstack((inverse_umat.iloc[np.array([u for _ in range(len(i))])].values.reshape(-1, 1),\n",
    "                                inverse_imat.iloc[i.reshape(-k,)].values.reshape(-1, 1), \n",
    "                                np.asarray(self.prediction_matrix.T[np.array([u for _ in range(len(i))]), i]).reshape(-1, 1)))))\n",
    "        df = pd.DataFrame(result[1:], columns = ['userId', 'movieId', 'prediction'])\n",
    "        return self.spark.createDataFrame(df)\n",
    "\n",
    "\n",
    "    def _preprocess(self, X, fit):\n",
    "        \"\"\"[preprocessing function before training and predicting]\n",
    "        Args:\n",
    "            X (Pyspark DataFrame): [training/predicting set]\n",
    "            fit (bool): [if it is on training stage or not]\n",
    "        Raises:\n",
    "            NotImplementedError: [if not User base or Item base]\n",
    "        Returns:\n",
    "            sparse.csr_matrix: [if on training stage],\n",
    "            numpy.array: [row and columns in np.array if on prediction stage]\n",
    "        \"\"\"        \n",
    "        if fit:\n",
    "            self.idxer = indexTransformer(self.usercol, self.itemcol)\n",
    "            self.idxer.fit(X)\n",
    "            _X = self.idxer.transform(X)\\\n",
    "                            .select(F.col(self.usercol+'_idx').alias(self.usercol), \n",
    "                                    F.col(self.itemcol+'_idx').alias(self.itemcol), \n",
    "                                    F.col(self.ratingcol))\n",
    "            _X = _X.toPandas().values\n",
    "            if self.base == 'user':\n",
    "                row = _X[:, 0].astype(int)\n",
    "                col = _X[:, 1].astype(int)\n",
    "                data = _X[:, 2].astype(float)\n",
    "            elif self.base == 'item':\n",
    "                row = _X[:, 1].astype(int)\n",
    "                col = _X[:, 0].astype(int)\n",
    "                data = _X[:, 2].astype(float)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            return sparse.csr_matrix((data, (row, col)))\n",
    "        else:\n",
    "            _X = self.idxer.transform(X).select(self.usercol+'_idx', self.itemcol+'_idx').toPandas().values\n",
    "            if self.base == 'user':\n",
    "                row = _X[:, 0].astype(int)\n",
    "                col = _X[:, 1].astype(int)\n",
    "            elif self.base == 'item':\n",
    "                row = _X[:, 1].astype(int)\n",
    "                col = _X[:, 0].astype(int)\n",
    "            else:\n",
    "                raise NotImplementedError\n",
    "            return row, col\n",
    "\n",
    "    def _pearson_corr(self, A):\n",
    "        \"\"\"[generating pearson corretion matrix for the model when training]\n",
    "        Args:\n",
    "            A (sparse.csr_matrix): [the training set in sparse matrix form with entries of ratings]\n",
    "        Returns:\n",
    "            sparse.csr_matrix: [the pearson correlation matrix in sparse form]\n",
    "        \"\"\"        \n",
    "        n = A.shape[1]\n",
    "        \n",
    "        rowsum = A.sum(1)\n",
    "        centering = rowsum.dot(rowsum.T) / n\n",
    "        C = (A.dot(A.T) - centering) / (n - 1)\n",
    "        \n",
    "        d = np.diag(C)\n",
    "        coeffs = C / np.sqrt(np.outer(d, d))\n",
    "        return np.array(np.nan_to_num(coeffs)) - np.eye(A.shape[0])\n",
    "    def _get_predict(self):\n",
    "        \"\"\"[generating prediction matrix]\n",
    "        Returns:\n",
    "            sparse.csr_matrix: [the prediction matrix in sparse form]\n",
    "        \"\"\"        \n",
    "        mu_iarray = np.array(np.nan_to_num(self.X.sum(1) / (self.X != 0).sum(1))).reshape(-1)\n",
    "        mu_imat = np.vstack([mu_iarray for _ in range(self.X.shape[1])]).T\n",
    "        x = self.X.copy()\n",
    "        x[x==0] = np.NaN\n",
    "        diff = np.nan_to_num(x-mu_imat)\n",
    "        sim_norm_mat = abs(self.similarity_matrix).dot((diff!=0).astype(int))\n",
    "        w = self.similarity_matrix.dot(diff) / sim_norm_mat\n",
    "        w = np.nan_to_num(w)\n",
    "        return mu_imat + w\n",
    "class indexTransformer():\n",
    "    \"\"\"[helper class for memory based model]\n",
    "    \"\"\"    \n",
    "    def __init__(self, usercol='userId', itemcol='movieId', ratingcol='rating'):\n",
    "        \"\"\"[the index transformer for matrix purpose]\n",
    "        Args:\n",
    "            usercol (str, optional): [user column name]. Defaults to 'userId'.\n",
    "            itemcol (str, optional): [item column name]. Defaults to 'movieId'.\n",
    "        \"\"\"        \n",
    "        self.usercol = usercol\n",
    "        self.itemcol = itemcol\n",
    "        self.ratingcol = ratingcol\n",
    "        self.u_indxer =  M.feature.StringIndexer(inputCol=usercol, \n",
    "                                                outputCol=usercol+'_idx', \n",
    "                                                handleInvalid = 'skip')\n",
    "        self.i_indxer = M.feature.StringIndexer(inputCol=itemcol, \n",
    "                                                outputCol=itemcol+'_idx', \n",
    "                                                handleInvalid = 'skip')\n",
    "        self.X = None\n",
    "    def fit(self, X):\n",
    "        \"\"\"[to train the transformer]\n",
    "        Args:\n",
    "            X (Pyspark DataFrame): [the DataFrame for training]\n",
    "        \"\"\"        \n",
    "        self.X = X\n",
    "        self.u_indxer = self.u_indxer.fit(self.X)\n",
    "        self.i_indxer = self.i_indxer.fit(self.X)\n",
    "        return\n",
    "    def transform(self, X):\n",
    "        \"\"\"[to transform the DataFrame]\n",
    "        Args:\n",
    "            X (Pyspark DataFrame): [the DataFrame needs to be transformed]\n",
    "        Returns:\n",
    "            Pyspark DataFrame: [the transformed DataFrame with index]\n",
    "        \"\"\"        \n",
    "        X_ = self.u_indxer.transform(X)\n",
    "        X_ = self.i_indxer.transform(X_)\n",
    "        return X_.orderBy([self.usercol+'_idx', self.itemcol+'_idx'])\n",
    "    \n",
    "    def fit_transform(self, X):\n",
    "        \"\"\"[combining fit and transform]\n",
    "        Args:\n",
    "            X (Pyspark DataFrame): [the DataFrame needs to be trained and transformed]\n",
    "        Returns:\n",
    "            Pyspark DataFrame: [the transformed DataFrame with index]\n",
    "        \"\"\"        \n",
    "        self.fit(X)\n",
    "        return self.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Als():\n",
    "    \"\"\"[the predictor for Pyspark ALS]\n",
    "    \"\"\"\n",
    "    def __init__(self, userCol, itemCol, ratingCol, regParam, seed, rank):\n",
    "        self.userCol = userCol\n",
    "        self.itemCol = itemCol\n",
    "        self.ratingCol = ratingCol\n",
    "        self.model =None\n",
    "        self.als = ALS(userCol=userCol,\n",
    "                itemCol=itemCol,\n",
    "                ratingCol=ratingCol,\n",
    "                coldStartStrategy=\"drop\",\n",
    "                nonnegative=True,\n",
    "                regParam=regParam,\n",
    "                seed=seed,\n",
    "                rank=rank)\n",
    "    def fit(self, _X):\n",
    "        \"\"\"[function to train parameter of predictor]\n",
    "        Args:\n",
    "            _X (Pyspark DataFrame): [training set]\n",
    "        \"\"\"\n",
    "        X = self._preprocess(_X)\n",
    "        self.model = self.als.fit(X)\n",
    "\n",
    "    def predict(self, _X):\n",
    "        \"\"\"[function to make predict over test set]\n",
    "        Args:\n",
    "            _X (Pyspark DataFrame): [test set]\n",
    "        Returns:\n",
    "            Pyspark DataFrame: [DataFrame with 'prediction' column which has the predicting value]\n",
    "        \"\"\"        \n",
    "        X = self._preprocess(_X)\n",
    "        return self.model.transform(X)\n",
    "\n",
    "    def recommend(self, X, numItems):\n",
    "        return self.model.recommendForUserSubset(X, numItems)\\\n",
    "                .select(self.userCol, F.explode('recommendations').alias('recommendations'))\\\n",
    "                .select(self.userCol, 'recommendations.*')\\\n",
    "                .select(self.userCol, self.itemCol, F.col(self.ratingCol).alias('prediction'))\n",
    "    def _preprocess(self, _X):\n",
    "        \"\"\"[preprocess the input dataset]\n",
    "        Args:\n",
    "            _X (Pyspark DataFrame): [the training or test set]\n",
    "        Returns:\n",
    "            Pyspark DataFrame: [the preprocessed DataFrame]\n",
    "        \"\"\"\n",
    "        cast_int = lambda df: df.select([F.col(c).cast('int') for c in [self.userCol, self.itemCol]] + \\\n",
    "                                [F.col(self.ratingCol).cast('float')])\n",
    "        return cast_int(_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class code_start():\n",
    "    def __init__(self, movie):\n",
    "        movie_copy = movie.withColumn(\"year\",F.regexp_extract(movie.title,r\"(\\d{4})\",0).cast(T.IntegerType()))\n",
    "        movie_copy = movie_copy.withColumn(\"genre\",F.explode(F.split(movie.genres,pattern=\"\\|\")))\n",
    "        movie_copy = movie_copy.select(\"movieId\",\"title\",\"genre\",\"year\")\n",
    "        genres = movie_copy.select(\"genre\").distinct().toPandas()['genre'].tolist()\n",
    "\n",
    "        sample_copy = sample.select(\"userId\",\"movieId\")\n",
    "\n",
    "        total = sample_copy.join(movie_copy,[\"movieId\"],'left')\n",
    "        popular = total.groupby(\"movieId\").count().sort(\"count\",ascending=False)\n",
    "        \n",
    "        self.movie = movie\n",
    "        self.popular = popular\n",
    "        \n",
    "    def recommend(self):\n",
    "        return self.popular.select(\"movieId\").limit(50).select('movieId')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class wide_deep():\n",
    "    def __init__(self,wide_cols='genres',\n",
    "                    deep_cols=['userId', 'movieId'],\n",
    "                    target_col = 'rating',\n",
    "                    deep_embs=[64, 64],\n",
    "                    deep_hidden=[64,32,16],\n",
    "                    deep_dropout=[0.1, 0.1, .1],\n",
    "                    deep_bachnorm=True):\n",
    "        self.wide = None\n",
    "        self.deep = None\n",
    "        self.deep_hidden = deep_hidden\n",
    "        self.deep_dropout = deep_dropout\n",
    "        self.deep_bachnorm = deep_bachnorm\n",
    "        self.model = None\n",
    "        self.wide_cols = wide_cols\n",
    "        self.deep_cols = deep_cols\n",
    "        self.embs = [(col, dim) for col, dim in zip(deep_cols, deep_embs)]\n",
    "        self.wide_preprocessor = self._genre_preprocessor(wide_cols)\n",
    "        self.deep_preprocessor = DensePreprocessor(embed_cols=self.embs)\n",
    "        self.target_col = target_col\n",
    "\n",
    "\n",
    "    def fit(self, train, n_epochs=10, batch_size=128, val_split=.1, verbose = True):\n",
    "        X, y = train.drop(self.target_col, axis = 1), train[self.target_col].values\n",
    "        wide_feature = self.wide_preprocessor.fit_transform(X)\n",
    "        deep_feature = self.deep_preprocessor.fit_transform(X)\n",
    "        self.wide = Wide(wide_dim=np.unique(wide_feature).shape[0], pred_dim=1)\n",
    "        self.deep = DeepDense(hidden_layers=self.deep_hidden, dropout=self.deep_dropout,\n",
    "                      batchnorm=self.deep_bachnorm,\n",
    "                      deep_column_idx=self.deep_preprocessor.deep_column_idx,\n",
    "                      embed_input=self.deep_preprocessor.embeddings_input)\n",
    "        self.model =  WideDeep(wide=self.wide, deepdense=self.deep)\n",
    "        wide_opt = torch.optim.Adam(self.model.wide.parameters(), lr=0.01)\n",
    "        deep_opt = RAdam(self.model.deepdense.parameters())\n",
    "        wide_sch = torch.optim.lr_scheduler.StepLR(wide_opt, step_size=3)\n",
    "        deep_sch = torch.optim.lr_scheduler.StepLR(deep_opt, step_size=5)\n",
    "        callbacks = [\n",
    "                        LRHistory(n_epochs=n_epochs),\n",
    "                        EarlyStopping(patience=5),\n",
    "                        ModelCheckpoint(filepath=\"model_weights/wd_out\"),\n",
    "                    ]\n",
    "        optimizers = {\"wide\": wide_opt, \"deepdense\": deep_opt}\n",
    "        schedulers = {\"wide\": wide_sch, \"deepdense\": deep_sch}\n",
    "        initializers = {\"wide\": KaimingNormal, \"deepdense\": XavierNormal}\n",
    "        self.model.compile(method='regression',\n",
    "                            optimizers=optimizers,\n",
    "                        lr_schedulers=schedulers,\n",
    "                        initializers=initializers,\n",
    "                        callbacks=callbacks,\n",
    "                        verbose=verbose)\n",
    "        self.model.fit(X_wide=wide_feature, \n",
    "                  X_deep=deep_feature, \n",
    "                  target=y, \n",
    "                  n_epochs=n_epochs, \n",
    "                  batch_size=batch_size, \n",
    "                  val_split=val_split,)\n",
    "    def load_pretrained(self, train, fp, device):\n",
    "        X = train.copy()\n",
    "        if type(self.wide_cols) == str:\n",
    "            wide_feature = self.wide_preprocessor.fit_transform(X[[self.wide_cols]])\n",
    "        else:\n",
    "            wide_feature = self.wide_preprocessor.fit_transform(X[self.wide_cols])\n",
    "        deep_feature = self.deep_preprocessor.fit_transform(X[self.deep_cols])\n",
    "        self.wide = Wide(wide_dim=np.unique(wide_feature).shape[0], pred_dim=1)\n",
    "        self.deep = DeepDense(hidden_layers=self.deep_hidden, dropout=self.deep_dropout,\n",
    "                      batchnorm=self.deep_bachnorm,\n",
    "                      deep_column_idx=self.deep_preprocessor.deep_column_idx,\n",
    "                      embed_input=self.deep_preprocessor.embeddings_input)\n",
    "        self.model =  torch.load(fp,  map_location=torch.device(device))\n",
    "        \n",
    "    def predict(self, test):\n",
    "        X = test.copy()\n",
    "        wide_feature = self.wide_preprocessor.transform(X)\n",
    "        deep_feature = self.deep_preprocessor.transform(X)\n",
    "        return self.model.predict(X_wide=wide_feature, X_deep=deep_feature)\n",
    "\n",
    "    def _genre_preprocessor(self, genre_feat):\n",
    "        dense_layer = lambda X: X.toarray()\n",
    "        genre_transformer = Pipeline(steps=[\n",
    "                ('tokenizer', CountVectorizer()),\n",
    "                ('dense', FunctionTransformer(dense_layer, validate=False))   \n",
    "        ])\n",
    "        preproc = ColumnTransformer(transformers=[('genre', genre_transformer, genre_feat),])\n",
    "        return preproc\n",
    "\n",
    "\n",
    "    def _deep_preprocessor(self,embs):\n",
    "        return DensePreprocessor(embed_cols=embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_based = Memory_based_CF(spark, base='item', usercol='userId', itemcol='movieId', ratingcol='rating')\n",
    "als = Als(userCol='userId', itemCol='movieId', ratingCol='rating', regParam=.15, seed=0, rank=10)\n",
    "\n",
    "item_based.fit(sample_train)\n",
    "als.fit(sample_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
